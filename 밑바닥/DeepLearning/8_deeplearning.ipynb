{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 가중치 매개변수\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from layer import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3112022407206805\n",
      "=== epoch:1, train acc:0.088, test acc:0.096 ===\n",
      "train loss:2.2786109733748416\n",
      "train loss:2.313928288455426\n",
      "train loss:2.2942072394793787\n",
      "train loss:2.298415662549137\n",
      "train loss:2.2578003143131276\n",
      "train loss:2.271179934879758\n",
      "train loss:2.299687051624284\n",
      "train loss:2.275271211266005\n",
      "train loss:2.273287044132027\n",
      "train loss:2.2454297178471694\n",
      "train loss:2.272756200633782\n",
      "train loss:2.290254198811464\n",
      "train loss:2.251587496946292\n",
      "train loss:2.2428105659201183\n",
      "train loss:2.249216787397556\n",
      "train loss:2.2411361828072986\n",
      "train loss:2.25741606715429\n",
      "train loss:2.198990091807112\n",
      "train loss:2.1893470658783944\n",
      "train loss:2.1837183287206177\n",
      "train loss:2.1686512050808657\n",
      "train loss:2.191055018835646\n",
      "train loss:2.129059395603528\n",
      "train loss:2.1341818650446185\n",
      "train loss:2.0781336886859756\n",
      "train loss:2.101825544465716\n",
      "train loss:2.06537145590717\n",
      "train loss:2.1009854736973863\n",
      "train loss:2.136986531051401\n",
      "train loss:2.1343158706639698\n",
      "train loss:2.0097316009377026\n",
      "train loss:2.0237458150901078\n",
      "train loss:2.030012090636746\n",
      "train loss:1.9658792211413552\n",
      "train loss:1.9947772657979164\n",
      "train loss:1.929570168548208\n",
      "train loss:1.9315112890479071\n",
      "train loss:1.9394896329435385\n",
      "train loss:1.8061355234528893\n",
      "train loss:1.9267532096860167\n",
      "train loss:1.9285888044840647\n",
      "train loss:1.9722829027283175\n",
      "train loss:1.8823686742790604\n",
      "train loss:1.8866444260104853\n",
      "train loss:1.7130522985490502\n",
      "train loss:1.8438451630561448\n",
      "train loss:1.9247329694851123\n",
      "train loss:1.9385714635635742\n",
      "train loss:1.8989055832034034\n",
      "train loss:1.8340461675664324\n",
      "train loss:1.8587665857806743\n",
      "train loss:1.83863397082956\n",
      "train loss:1.891478885804316\n",
      "train loss:1.8047371645381103\n",
      "train loss:1.796189620145683\n",
      "train loss:1.7794252979975027\n",
      "train loss:1.7118089417512907\n",
      "train loss:1.7263023676846898\n",
      "train loss:1.9334299273633966\n",
      "train loss:1.7925519921259596\n",
      "train loss:1.8299650097116826\n",
      "train loss:1.7806438915065383\n",
      "train loss:1.8244488008348643\n",
      "train loss:1.7404502822027643\n",
      "train loss:1.7501002785364792\n",
      "train loss:1.7064192493898667\n",
      "train loss:1.765752818271973\n",
      "train loss:1.7386642923482063\n",
      "train loss:1.9144262185817809\n",
      "train loss:1.7716564473231475\n",
      "train loss:1.70200766227087\n",
      "train loss:1.6949784520363644\n",
      "train loss:1.7509968082679805\n",
      "train loss:1.5174926995557436\n",
      "train loss:1.5658494078891456\n",
      "train loss:1.7524344054624397\n",
      "train loss:1.5354929442252665\n",
      "train loss:1.6632189427073243\n",
      "train loss:1.5929246066313192\n",
      "train loss:1.7358540599287124\n",
      "train loss:1.7898136786184105\n",
      "train loss:1.76599809014128\n",
      "train loss:1.6057762449727528\n",
      "train loss:1.708991535019765\n",
      "train loss:1.4906055856327245\n",
      "train loss:1.6753055673318926\n",
      "train loss:1.688936978312627\n",
      "train loss:1.457996311198447\n",
      "train loss:1.5276255458254409\n",
      "train loss:1.7374497123367718\n",
      "train loss:1.6727188275080553\n",
      "train loss:1.6394300317615498\n",
      "train loss:1.6057999991191074\n",
      "train loss:1.5579969271567478\n",
      "train loss:1.6345314722653124\n",
      "train loss:1.6225707636655609\n",
      "train loss:1.721947865723881\n",
      "train loss:1.7772545641138024\n",
      "train loss:1.540499050287837\n",
      "train loss:1.6001848900038385\n",
      "train loss:1.6177021371626283\n",
      "train loss:1.5298537936872494\n",
      "train loss:1.4247505207763864\n",
      "train loss:1.5457551981369013\n",
      "train loss:1.4423121712963285\n",
      "train loss:1.3378495770748144\n",
      "train loss:1.6280084606872873\n",
      "train loss:1.350959165096041\n",
      "train loss:1.3861330999456505\n",
      "train loss:1.4711104220836868\n",
      "train loss:1.426634102831085\n",
      "train loss:1.3929084535764338\n",
      "train loss:1.4611602327375874\n",
      "train loss:1.4732544931675802\n",
      "train loss:1.4896859204704018\n",
      "train loss:1.547837386040104\n",
      "train loss:1.487273194592256\n",
      "train loss:1.7110491288300964\n",
      "train loss:1.4913685572350912\n",
      "train loss:1.4980756453542374\n",
      "train loss:1.5378246303552645\n",
      "train loss:1.3852474526246488\n",
      "train loss:1.3528942015487186\n",
      "train loss:1.420643273734687\n",
      "train loss:1.3656540605047396\n",
      "train loss:1.652296651987539\n",
      "train loss:1.472810999640841\n",
      "train loss:1.392409893206692\n",
      "train loss:1.4413886074141524\n",
      "train loss:1.3957126807993077\n",
      "train loss:1.4298327006400762\n",
      "train loss:1.6296136881130723\n",
      "train loss:1.609247573753597\n",
      "train loss:1.383561411395708\n",
      "train loss:1.5154454037632246\n",
      "train loss:1.5220236765333652\n",
      "train loss:1.359269888214992\n",
      "train loss:1.3572263400800155\n",
      "train loss:1.327869842404751\n",
      "train loss:1.3482514274851731\n",
      "train loss:1.5628129494500778\n",
      "train loss:1.3042110231116737\n",
      "train loss:1.4226135063711263\n",
      "train loss:1.3955966459073066\n",
      "train loss:1.3623122295039556\n",
      "train loss:1.27534329450536\n",
      "train loss:1.4776589180572883\n",
      "train loss:1.5615619211010616\n",
      "train loss:1.3228701480985126\n",
      "train loss:1.319112942125983\n",
      "train loss:1.3682067746092066\n",
      "train loss:1.3322219034376175\n",
      "train loss:1.4631592970812597\n",
      "train loss:1.6360844599271118\n",
      "train loss:1.4065028864918534\n",
      "train loss:1.255748975305461\n",
      "train loss:1.5417177477333441\n",
      "train loss:1.3828740138466333\n",
      "train loss:1.4584031846603542\n",
      "train loss:1.6420577843053907\n",
      "train loss:1.2910627795110272\n",
      "train loss:1.449019829697631\n",
      "train loss:1.215916027856941\n",
      "train loss:1.6267652788948865\n",
      "train loss:1.2762370903546003\n",
      "train loss:1.4573359996312178\n",
      "train loss:1.354823901771884\n",
      "train loss:1.329827948027407\n",
      "train loss:1.330375598766256\n",
      "train loss:1.3389419782231187\n",
      "train loss:1.4542386268069836\n",
      "train loss:1.52389534283681\n",
      "train loss:1.4564288248925905\n",
      "train loss:1.3786283952882743\n",
      "train loss:1.4847551977595033\n",
      "train loss:1.4309956958408407\n",
      "train loss:1.1992514176363025\n",
      "train loss:1.4829447271152396\n",
      "train loss:1.3867797949261516\n",
      "train loss:1.2558580292924826\n",
      "train loss:1.3027912643127058\n",
      "train loss:1.2628269915497665\n",
      "train loss:1.4563628674212503\n",
      "train loss:1.2400599174033269\n",
      "train loss:1.385456021312304\n",
      "train loss:1.5737713779176838\n",
      "train loss:1.374125096919631\n",
      "train loss:1.3101271200216908\n",
      "train loss:1.2923684278687957\n",
      "train loss:1.4077240757161436\n",
      "train loss:1.4939939751195375\n",
      "train loss:1.3377097524155985\n",
      "train loss:1.4931228844574926\n",
      "train loss:1.3554534043480866\n",
      "train loss:1.3685306486695081\n",
      "train loss:1.4506638680469126\n",
      "train loss:1.2772345916739658\n",
      "train loss:1.2688608732724802\n",
      "train loss:1.263718921011039\n",
      "train loss:1.3191075999276782\n",
      "train loss:1.2072277461517174\n",
      "train loss:1.416903794478709\n",
      "train loss:1.2879972651582667\n",
      "train loss:1.3511156460663627\n",
      "train loss:1.3959928228726688\n",
      "train loss:1.2030605507372254\n",
      "train loss:1.3934133074975827\n",
      "train loss:1.3796902068736627\n",
      "train loss:1.347784489294395\n",
      "train loss:1.362693953457597\n",
      "train loss:1.3195090336942354\n",
      "train loss:1.1956779443673573\n",
      "train loss:1.2880171659566912\n",
      "train loss:1.2746872803599822\n",
      "train loss:1.2910505945228818\n",
      "train loss:1.241773581802049\n",
      "train loss:1.2330349619110066\n",
      "train loss:1.2880430573753268\n",
      "train loss:1.105178128498781\n",
      "train loss:1.2570382077988902\n",
      "train loss:1.317501505755043\n",
      "train loss:1.3200600474940343\n",
      "train loss:1.2477508979186762\n",
      "train loss:1.2961006325036954\n",
      "train loss:1.4170912676919847\n",
      "train loss:1.0954487220344304\n",
      "train loss:1.3316054298210687\n",
      "train loss:1.11010536272252\n",
      "train loss:1.2655948110623296\n",
      "train loss:1.2267056691331457\n",
      "train loss:1.2825791155258783\n",
      "train loss:1.229812060487531\n",
      "train loss:1.0512230516260825\n",
      "train loss:1.2128140376742422\n",
      "train loss:1.2833684054047045\n",
      "train loss:1.183747885917495\n",
      "train loss:1.335340885165155\n",
      "train loss:1.3309437092787895\n",
      "train loss:1.2190351764606024\n",
      "train loss:1.217749092675277\n",
      "train loss:1.1718198693112887\n",
      "train loss:1.2533093774734894\n",
      "train loss:1.2733749777882382\n",
      "train loss:1.2786444953138165\n",
      "train loss:1.2078421304498113\n",
      "train loss:1.2631754879404524\n",
      "train loss:1.359690373056575\n",
      "train loss:1.1377082415494142\n",
      "train loss:1.3705276751717685\n",
      "train loss:1.0588454203016515\n",
      "train loss:1.3012979429338631\n",
      "train loss:1.27905211988135\n",
      "train loss:0.9852012920368775\n",
      "train loss:1.1999779520693001\n",
      "train loss:1.3431568200222934\n",
      "train loss:1.2067649897876505\n",
      "train loss:1.1866580391459798\n",
      "train loss:1.1070066973514918\n",
      "train loss:1.2401100931575675\n",
      "train loss:1.3113396264249428\n",
      "train loss:1.1043422342475646\n",
      "train loss:1.1787328004273168\n",
      "train loss:1.4830582329233122\n",
      "train loss:1.3108883782963958\n",
      "train loss:1.334308173818532\n",
      "train loss:1.119916139017388\n",
      "train loss:1.0377908559770346\n",
      "train loss:1.151294131859163\n",
      "train loss:1.3346077208157134\n",
      "train loss:1.1094152022961157\n",
      "train loss:1.0999991323898777\n",
      "train loss:1.206448344586532\n",
      "train loss:1.2446910733481205\n",
      "train loss:1.2810294126862276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.3697905138598556\n",
      "train loss:1.4661519973997161\n",
      "train loss:1.2980817458305862\n",
      "train loss:1.3282166144776042\n",
      "train loss:1.155206961502526\n",
      "train loss:1.191033721932843\n",
      "train loss:1.2016863448043094\n",
      "train loss:1.2156137715309816\n",
      "train loss:1.112579679326715\n",
      "train loss:1.0791409968838779\n",
      "train loss:1.1995979515921877\n",
      "train loss:1.1618283477173283\n",
      "train loss:1.4544675195896937\n",
      "train loss:1.1687169448198693\n",
      "train loss:1.3683962277378883\n",
      "train loss:1.2493714860006762\n",
      "train loss:1.1592577538516486\n",
      "train loss:1.1711921314107785\n",
      "train loss:1.210845997102705\n",
      "train loss:1.1951809235930857\n",
      "train loss:1.2941035734535098\n",
      "train loss:1.1697011494284042\n",
      "train loss:1.1968292694073803\n",
      "train loss:1.1999396059065757\n",
      "train loss:1.285775174391612\n",
      "train loss:1.095110710605791\n",
      "train loss:1.0620912013420591\n",
      "train loss:1.2228522149775514\n",
      "train loss:0.9836593469890927\n",
      "train loss:1.050023131022546\n",
      "train loss:1.1613015745822093\n",
      "train loss:1.0510200159722065\n",
      "train loss:1.2220409474293696\n",
      "train loss:1.113444686506316\n",
      "train loss:1.193159416427455\n",
      "train loss:1.0851025978469189\n",
      "train loss:1.2602751812140005\n",
      "train loss:0.9947514675815254\n",
      "train loss:1.0912147445327205\n",
      "train loss:1.2557942028278726\n",
      "train loss:1.089083655194201\n",
      "train loss:1.1533048142834412\n",
      "train loss:1.1510856160562712\n",
      "train loss:1.1139991317759232\n",
      "train loss:1.1496923425886891\n",
      "train loss:0.9804251495561815\n",
      "train loss:1.2677104819766962\n",
      "train loss:1.0656236239263317\n",
      "train loss:1.0951984122468206\n",
      "train loss:1.1091962699611637\n",
      "train loss:0.9385817761835589\n",
      "train loss:1.194881197881145\n",
      "train loss:1.196533798227497\n",
      "train loss:1.1799556507483033\n",
      "train loss:1.1255804322186211\n",
      "train loss:1.2530477294578324\n",
      "train loss:1.273742525887859\n",
      "train loss:1.0933342968711979\n",
      "train loss:1.250441504385285\n",
      "train loss:1.22473151772929\n",
      "train loss:1.197935473010407\n",
      "train loss:1.0997851160040495\n",
      "train loss:1.019484028071365\n",
      "train loss:1.2681619922102134\n",
      "train loss:1.0837869068762864\n",
      "train loss:1.1109086286149341\n",
      "train loss:1.1620349056739048\n",
      "train loss:1.105945604427475\n",
      "train loss:1.1681149122237295\n",
      "train loss:1.224557434584432\n",
      "train loss:1.1074939748438262\n",
      "train loss:1.1180263752507587\n",
      "train loss:1.046978998597566\n",
      "train loss:1.0631482958090468\n",
      "train loss:1.1578609111244798\n",
      "train loss:1.1474595253437965\n",
      "train loss:1.1399331118406453\n",
      "train loss:1.0321998156629462\n",
      "train loss:1.2189115559253478\n",
      "train loss:1.1413471991922144\n",
      "train loss:0.9493940437340455\n",
      "train loss:1.228188292008568\n",
      "train loss:1.1015404104888349\n",
      "train loss:1.0343470053610173\n",
      "train loss:0.9994947603112165\n",
      "train loss:0.9682300042467158\n",
      "train loss:0.9647680000220934\n",
      "train loss:1.115719999980399\n",
      "train loss:0.9091565663066313\n",
      "train loss:1.0050758520220846\n",
      "train loss:1.0007743318888596\n",
      "train loss:1.1680083700967483\n",
      "train loss:1.051202872774851\n",
      "train loss:1.1841025304989976\n",
      "train loss:1.2142819671360159\n",
      "train loss:1.0272660317384341\n",
      "train loss:1.1692546789355962\n",
      "train loss:1.2551371463790375\n",
      "train loss:1.2287278022460653\n",
      "train loss:1.074933539063926\n",
      "train loss:1.2334886449392906\n",
      "train loss:1.1360422889083248\n",
      "train loss:1.2505331886183961\n",
      "train loss:1.0535289667079175\n",
      "train loss:0.9602544158746942\n",
      "train loss:1.0429199764604062\n",
      "train loss:1.1441325309731234\n",
      "train loss:1.1233835233647387\n",
      "train loss:1.0115844189032033\n",
      "train loss:1.2462378264949625\n",
      "train loss:1.1558978562684048\n",
      "train loss:0.9236246236443071\n",
      "train loss:1.1655317234620002\n",
      "train loss:1.186693804762398\n",
      "train loss:1.1090226750568857\n",
      "train loss:1.0849225404320813\n",
      "train loss:1.0492927417629856\n",
      "train loss:1.088288536615758\n",
      "train loss:1.0701357674959244\n",
      "train loss:1.2316973311902908\n",
      "train loss:1.1726868897621925\n",
      "train loss:0.9115930778331753\n",
      "train loss:1.1082055437891476\n",
      "train loss:1.2044552643008675\n",
      "train loss:1.1084647109808494\n",
      "train loss:1.0880427840955333\n",
      "train loss:0.9512565979163474\n",
      "train loss:1.129410467350614\n",
      "train loss:1.189870130522495\n",
      "train loss:1.0131436230737407\n",
      "train loss:1.0849328131578349\n",
      "train loss:1.2006919874427693\n",
      "train loss:0.9963046464083376\n",
      "train loss:1.1913892366674341\n",
      "train loss:1.215113209007858\n",
      "train loss:1.1470267828524516\n",
      "train loss:1.173566459383185\n",
      "train loss:1.1151127621576784\n",
      "train loss:1.0463195615497223\n",
      "train loss:1.0323884031815558\n",
      "train loss:1.117091307036413\n",
      "train loss:1.2973762669916094\n",
      "train loss:1.2680201164996188\n",
      "train loss:1.119093831167214\n",
      "train loss:1.1477846514344463\n",
      "train loss:1.1292767837567832\n",
      "train loss:1.1732373183274432\n",
      "train loss:1.0189705596130658\n",
      "train loss:1.1020362818040879\n",
      "train loss:1.1595798716017884\n",
      "train loss:1.0425151695754664\n",
      "train loss:1.1391214606588678\n",
      "train loss:1.1529956168617748\n",
      "train loss:1.1497958179118408\n",
      "train loss:1.2909447386147\n",
      "train loss:1.0571328679664553\n",
      "train loss:1.0718228687091642\n",
      "train loss:1.0284654889103921\n",
      "train loss:1.1411256098003921\n",
      "train loss:1.1219873640451339\n",
      "train loss:1.0006816094047708\n",
      "train loss:0.9808438921850818\n",
      "train loss:1.112954139440646\n",
      "train loss:0.9446893664261709\n",
      "train loss:1.1647443213935198\n",
      "train loss:1.1160177267904159\n",
      "train loss:1.1749519883725577\n",
      "train loss:0.9564887537206324\n",
      "train loss:1.2345123262529827\n",
      "train loss:1.1530204972266125\n",
      "train loss:1.2242015235752965\n",
      "train loss:1.0454618079605036\n",
      "train loss:1.2456764617313787\n",
      "train loss:1.2024405547134855\n",
      "train loss:0.9690239712370367\n",
      "train loss:1.1444391133472813\n",
      "train loss:0.9628340701634731\n",
      "train loss:1.1941917280561782\n",
      "train loss:1.1825054831811928\n",
      "train loss:1.2857279407194822\n",
      "train loss:1.072968858361577\n",
      "train loss:1.0842625869984357\n",
      "train loss:0.9581349132821342\n",
      "train loss:1.0942184313605976\n",
      "train loss:1.0907244783177077\n",
      "train loss:1.22698848523049\n",
      "train loss:1.0999005568322295\n",
      "train loss:1.17869769023675\n",
      "train loss:1.076117427853703\n",
      "train loss:1.0569110951497978\n",
      "train loss:1.0389377417016072\n",
      "train loss:1.093024673592587\n",
      "train loss:1.1460168456805035\n",
      "train loss:0.9688932230628811\n",
      "train loss:0.8684150833364395\n",
      "train loss:1.161482191003994\n",
      "train loss:1.1868232981840234\n",
      "train loss:1.2177595124526448\n",
      "train loss:0.8708708665205471\n",
      "train loss:1.1390504448156595\n",
      "train loss:0.8888132931016781\n",
      "train loss:1.0716874864511194\n",
      "train loss:1.0654460480665426\n",
      "train loss:1.0870057944167895\n",
      "train loss:0.8846489584503432\n",
      "train loss:1.0668448717914478\n",
      "train loss:0.9675285745053196\n",
      "train loss:1.305938930886653\n",
      "train loss:1.05651919918283\n",
      "train loss:1.305872050807329\n",
      "train loss:1.1250583914720118\n",
      "train loss:1.2331459891446122\n",
      "train loss:0.9254954558061089\n",
      "train loss:1.0392459010764634\n",
      "train loss:0.9970737818720956\n",
      "train loss:1.263528318287198\n",
      "train loss:1.0458408672400135\n",
      "train loss:1.146741917141386\n",
      "train loss:1.2574445843585706\n",
      "train loss:0.9166821577766433\n",
      "train loss:1.1020433561150802\n",
      "train loss:1.1328007011636334\n",
      "train loss:0.9475369855201988\n",
      "train loss:0.9449064569009791\n",
      "train loss:1.1160837343856087\n",
      "train loss:0.9317567374796333\n",
      "train loss:1.1024305804559584\n",
      "train loss:0.8832206941972387\n",
      "train loss:1.0938303260030529\n",
      "train loss:1.1894070921170317\n",
      "train loss:1.0560615406134612\n",
      "train loss:1.1885070212630342\n",
      "train loss:1.077505559165653\n",
      "train loss:0.9630231518126905\n",
      "train loss:1.0197322232691854\n",
      "train loss:1.052648953765445\n",
      "train loss:1.2322862885106132\n",
      "train loss:1.1216812611819822\n",
      "train loss:0.9397266920524217\n",
      "train loss:1.1724952725959041\n",
      "train loss:1.0756204056106435\n",
      "train loss:1.1474514345173683\n",
      "train loss:1.0649241265273006\n",
      "train loss:1.0186739691277147\n",
      "train loss:1.1723018505885063\n",
      "train loss:1.2260893986907433\n",
      "train loss:1.0689390064737816\n",
      "train loss:1.0754031551243584\n",
      "train loss:1.1886506151404677\n",
      "train loss:1.021318494202869\n",
      "train loss:0.9523920526650719\n",
      "train loss:1.2235125526747455\n",
      "train loss:1.0779512415736472\n",
      "train loss:1.1708744151741148\n",
      "train loss:0.996895487963275\n",
      "train loss:1.1822132269690417\n",
      "train loss:1.0060074484019037\n",
      "train loss:1.0194780346750358\n",
      "train loss:1.070035690941378\n",
      "train loss:1.212481266562781\n",
      "train loss:1.1788728393003318\n",
      "train loss:1.2756227840823027\n",
      "train loss:1.013172469691153\n",
      "train loss:1.0472856950026328\n",
      "train loss:1.1929557001820645\n",
      "train loss:0.9439432505194396\n",
      "train loss:1.1723857954392511\n",
      "train loss:1.0932861468981798\n",
      "train loss:1.0778371023780857\n",
      "train loss:0.9576631649264642\n",
      "train loss:1.2155619627714227\n",
      "train loss:0.947378994791297\n",
      "train loss:1.061770829953745\n",
      "train loss:1.1863713787462737\n",
      "train loss:1.09194354372104\n",
      "train loss:1.0583833200531618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9541611989340385\n",
      "train loss:1.1597223431592456\n",
      "train loss:1.1543819519123877\n",
      "train loss:1.086466207237675\n",
      "train loss:1.0351191745070876\n",
      "train loss:1.0606122972784222\n",
      "train loss:0.9578277352604994\n",
      "train loss:1.2104970626148104\n",
      "train loss:1.172196991294953\n",
      "train loss:0.9507700514571609\n",
      "train loss:1.033136652079613\n",
      "train loss:1.147212575236419\n",
      "train loss:0.9221646183074524\n",
      "train loss:1.0663935786970162\n",
      "train loss:1.0711287542574937\n",
      "train loss:1.1062206132235406\n",
      "train loss:0.9389557648087754\n",
      "train loss:0.9650387712462983\n",
      "train loss:1.1548966931462874\n",
      "train loss:0.8642784850466989\n",
      "train loss:1.028146291911476\n",
      "train loss:0.9576659842070124\n",
      "train loss:1.1809253027003843\n",
      "train loss:1.058219129816411\n",
      "train loss:1.0418468208577756\n",
      "train loss:1.0083774315746996\n",
      "train loss:1.1861525193673457\n",
      "train loss:1.0055689156597998\n",
      "train loss:0.9116395704556238\n",
      "train loss:1.0029057452652324\n",
      "train loss:0.9625801985212263\n",
      "train loss:1.0263093500618008\n",
      "train loss:0.9995432040299922\n",
      "train loss:1.1494847362909812\n",
      "train loss:1.2144339425433417\n",
      "train loss:0.9906152466259655\n",
      "train loss:1.0326639810271034\n",
      "train loss:1.025348476734228\n",
      "train loss:0.9891920455981124\n",
      "train loss:1.1486256463638285\n",
      "train loss:0.8881900866959276\n",
      "train loss:1.0767746733467094\n",
      "train loss:1.007828965458586\n",
      "train loss:0.9751128025562004\n",
      "train loss:1.0500157965461054\n",
      "train loss:1.0818102864797832\n",
      "train loss:0.9819345993319554\n",
      "train loss:1.0448112491236679\n",
      "train loss:0.8992692267422481\n",
      "train loss:1.0964807529684066\n",
      "=== epoch:2, train acc:0.978, test acc:0.977 ===\n",
      "train loss:1.0082278582572217\n",
      "train loss:0.8842898016756443\n",
      "train loss:1.0800126954376494\n",
      "train loss:1.0503610376087287\n",
      "train loss:0.9250570503539944\n",
      "train loss:1.094459730113476\n",
      "train loss:1.2087424970783873\n",
      "train loss:0.9857139153433485\n",
      "train loss:0.9977480269000745\n",
      "train loss:1.102847529653251\n",
      "train loss:0.9602062983398989\n",
      "train loss:1.0568002328048645\n",
      "train loss:0.9000006044760877\n",
      "train loss:0.8939240195332039\n",
      "train loss:0.9893936994902309\n",
      "train loss:1.0401408173909057\n",
      "train loss:1.0209933659903516\n",
      "train loss:1.0747293066875896\n",
      "train loss:0.9793458300545663\n",
      "train loss:0.9865383444491745\n",
      "train loss:1.2101655775160476\n",
      "train loss:0.9488454985424309\n",
      "train loss:1.1583930838022263\n",
      "train loss:0.8785939372733003\n",
      "train loss:0.9647428858502584\n",
      "train loss:1.0202554319643826\n",
      "train loss:1.0998418402177454\n",
      "train loss:0.9860190079586149\n",
      "train loss:0.9759249382215706\n",
      "train loss:1.0199474124737093\n",
      "train loss:0.8699681575817345\n",
      "train loss:1.0739380957992435\n",
      "train loss:0.9507021702842211\n",
      "train loss:0.9935139038790453\n",
      "train loss:0.8784085494413942\n",
      "train loss:0.9677170264256074\n",
      "train loss:1.0224133012919476\n",
      "train loss:1.1134119885567018\n",
      "train loss:1.0627057707593817\n",
      "train loss:1.1188793656632448\n",
      "train loss:0.9459208203962973\n",
      "train loss:1.172525151662172\n",
      "train loss:1.1659731697199625\n",
      "train loss:1.1332477224909818\n",
      "train loss:0.8568554737054971\n",
      "train loss:0.9942098588483097\n",
      "train loss:1.1026658577821866\n",
      "train loss:1.1697694075786291\n",
      "train loss:1.2891862306008974\n",
      "train loss:0.9479104525457213\n",
      "train loss:1.0103345900921261\n",
      "train loss:1.0507386472846505\n",
      "train loss:0.9671670504882439\n",
      "train loss:1.0541193264017354\n",
      "train loss:0.9796450943791665\n",
      "train loss:1.0559764366406288\n",
      "train loss:0.9785106534274122\n",
      "train loss:1.0980836086730026\n",
      "train loss:1.0926772760844177\n",
      "train loss:0.9798544649330104\n",
      "train loss:0.98379093694399\n",
      "train loss:1.1773482960603276\n",
      "train loss:1.0582529791655773\n",
      "train loss:1.0887341414737788\n",
      "train loss:1.0762722572986594\n",
      "train loss:0.9996393965066597\n",
      "train loss:0.9962299063145647\n",
      "train loss:1.0699803359126436\n",
      "train loss:1.2588962735954679\n",
      "train loss:1.039295745628686\n",
      "train loss:0.9265122209066429\n",
      "train loss:1.0409947914966267\n",
      "train loss:0.9272935342620385\n",
      "train loss:1.0002702735845257\n",
      "train loss:1.017849989800421\n",
      "train loss:0.8966675850343765\n",
      "train loss:1.0188341303143367\n",
      "train loss:1.0093195679150238\n",
      "train loss:1.2480699541585252\n",
      "train loss:1.0957777204036674\n",
      "train loss:1.0341753289417535\n",
      "train loss:1.1829872067076115\n",
      "train loss:0.9872074260576338\n",
      "train loss:1.0309922884865095\n",
      "train loss:1.143796485399735\n",
      "train loss:0.9170025538838023\n",
      "train loss:1.0099379278260112\n",
      "train loss:1.2011784845393239\n",
      "train loss:0.9923723998814961\n",
      "train loss:1.0130676522754603\n",
      "train loss:1.2015263365846784\n",
      "train loss:1.075576381875571\n",
      "train loss:1.1781727320923665\n",
      "train loss:1.0431446196169947\n",
      "train loss:1.0554351245332523\n",
      "train loss:1.1607708648893655\n",
      "train loss:1.1222155531147648\n",
      "train loss:1.1082464127717142\n",
      "train loss:0.7875560718600347\n",
      "train loss:0.9543951652372197\n",
      "train loss:1.2056025221566324\n",
      "train loss:0.9863694428372702\n",
      "train loss:1.0978893273236192\n",
      "train loss:0.8494426776901907\n",
      "train loss:1.1220250856646188\n",
      "train loss:1.0011146675431088\n",
      "train loss:1.1533436677632496\n",
      "train loss:1.1160302808003486\n",
      "train loss:1.0783216781405445\n",
      "train loss:1.2622778584209733\n",
      "train loss:1.1296547660879157\n",
      "train loss:0.9029278796134359\n",
      "train loss:0.982334270833631\n",
      "train loss:0.9339354453974866\n",
      "train loss:0.8275032396489675\n",
      "train loss:1.061758363216886\n",
      "train loss:1.1093120865380688\n",
      "train loss:1.1795189924484004\n",
      "train loss:1.1064973069440167\n",
      "train loss:0.9334050278329596\n",
      "train loss:1.2120288630062799\n",
      "train loss:1.149192288084792\n",
      "train loss:1.0264185430789883\n",
      "train loss:0.994528598031758\n",
      "train loss:0.9881391369689212\n",
      "train loss:1.0029908142773676\n",
      "train loss:1.1649818270286612\n",
      "train loss:1.0065098617546109\n",
      "train loss:1.0622639384941663\n",
      "train loss:1.1540923244968573\n",
      "train loss:1.0017865030111195\n",
      "train loss:0.8563663615815511\n",
      "train loss:1.0122804173476074\n",
      "train loss:1.0357376001352114\n",
      "train loss:1.0000730096175248\n",
      "train loss:1.047404272212666\n",
      "train loss:0.9188914845302274\n",
      "train loss:1.0817697624206069\n",
      "train loss:1.0004179438233443\n",
      "train loss:0.9896636827914197\n",
      "train loss:0.8776181279876849\n",
      "train loss:0.9622707237713167\n",
      "train loss:1.1037918118929442\n",
      "train loss:0.9787556015462238\n",
      "train loss:1.084851899402109\n",
      "train loss:1.0681514224375697\n",
      "train loss:1.1552499471546411\n",
      "train loss:0.8839022779085636\n",
      "train loss:1.041875374319157\n",
      "train loss:1.0126690257138034\n",
      "train loss:1.0758225519473883\n",
      "train loss:1.1779215255024063\n",
      "train loss:1.2707252652360987\n",
      "train loss:1.0894816139794525\n",
      "train loss:1.0006996179196985\n",
      "train loss:0.8696638819488323\n",
      "train loss:0.8796037087180197\n",
      "train loss:0.9804695930049344\n",
      "train loss:1.002772844118991\n",
      "train loss:1.1753676126847765\n",
      "train loss:0.9117942262099555\n",
      "train loss:1.1117080450118193\n",
      "train loss:1.0896539426504435\n",
      "train loss:0.9583848776697877\n",
      "train loss:0.9839853529123039\n",
      "train loss:0.9430543967196887\n",
      "train loss:1.0753221528850778\n",
      "train loss:0.8881956537771194\n",
      "train loss:1.0112460689361595\n",
      "train loss:1.0799960951085588\n",
      "train loss:1.0287176999069219\n",
      "train loss:0.8598665379717788\n",
      "train loss:0.8100159406341644\n",
      "train loss:1.0262921916483003\n",
      "train loss:0.8319983030885255\n",
      "train loss:1.1355411362672971\n",
      "train loss:0.9334416667825087\n",
      "train loss:1.0703432654969145\n",
      "train loss:1.0087391250477082\n",
      "train loss:0.8958529407833231\n",
      "train loss:1.0315060902538058\n",
      "train loss:1.143402901667545\n",
      "train loss:0.9598117222129916\n",
      "train loss:0.8785951972977122\n",
      "train loss:1.0799315619292078\n",
      "train loss:1.124578957278847\n",
      "train loss:0.8170564992659155\n",
      "train loss:1.0401163807482263\n",
      "train loss:1.0819193599632337\n",
      "train loss:1.0610144078065387\n",
      "train loss:1.006859019812235\n",
      "train loss:0.9818203667775061\n",
      "train loss:1.09833263528515\n",
      "train loss:0.974404069440912\n",
      "train loss:0.8663760059613722\n",
      "train loss:1.060824306993387\n",
      "train loss:1.204827875410607\n",
      "train loss:1.2865760091038478\n",
      "train loss:0.9061528693048606\n",
      "train loss:0.911503505877409\n",
      "train loss:0.9558526323306347\n",
      "train loss:0.9169046439092843\n",
      "train loss:1.0252613504183286\n",
      "train loss:1.0779782309690398\n",
      "train loss:0.803729701194872\n",
      "train loss:1.330636251099744\n",
      "train loss:1.052643117406274\n",
      "train loss:1.2395406081169769\n",
      "train loss:0.9830096009725056\n",
      "train loss:1.131180490704451\n",
      "train loss:1.0687633777184846\n",
      "train loss:1.0704157781471013\n",
      "train loss:1.0977750799542016\n",
      "train loss:1.0611436963477112\n",
      "train loss:0.9105263018101662\n",
      "train loss:0.9753315780435652\n",
      "train loss:0.9693075421493775\n",
      "train loss:1.016837590477379\n",
      "train loss:0.9666399843900974\n",
      "train loss:0.8569662387935747\n",
      "train loss:1.0113310694161328\n",
      "train loss:1.0999373850271552\n",
      "train loss:0.9013997985899229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1334486257785654\n",
      "train loss:0.9909732830260718\n",
      "train loss:1.0445970232274036\n",
      "train loss:1.2427346046940055\n",
      "train loss:1.0802424792793384\n",
      "train loss:1.1411559735436345\n",
      "train loss:0.9107106946235541\n",
      "train loss:0.8496718913832395\n",
      "train loss:1.1376522225057735\n",
      "train loss:0.8949305180096312\n",
      "train loss:0.9034478979697512\n",
      "train loss:1.0320231134683089\n",
      "train loss:0.9132604596411091\n",
      "train loss:0.9570107906830903\n",
      "train loss:0.9574828482965141\n",
      "train loss:0.9279576762328173\n",
      "train loss:1.094839405177333\n",
      "train loss:0.9426065827266512\n",
      "train loss:1.0193231459463088\n",
      "train loss:1.0763126515643493\n",
      "train loss:1.119385946208728\n",
      "train loss:0.8851779792607491\n",
      "train loss:1.0072652679984435\n",
      "train loss:0.9699129490388796\n",
      "train loss:1.0342211400211638\n",
      "train loss:1.0818603163057694\n",
      "train loss:0.9383049992801059\n",
      "train loss:1.0509963831869322\n",
      "train loss:0.9356203182649174\n",
      "train loss:1.0679979154083628\n",
      "train loss:1.144373955883391\n",
      "train loss:0.9425188242324803\n",
      "train loss:0.9540872405460326\n",
      "train loss:1.0725746224901669\n",
      "train loss:1.0450295725404786\n",
      "train loss:1.0742059608927577\n",
      "train loss:0.9968832598757298\n",
      "train loss:0.9125279774856756\n",
      "train loss:0.9208753015443368\n",
      "train loss:1.0432039658684469\n",
      "train loss:1.0700314042519041\n",
      "train loss:0.9596992632985283\n",
      "train loss:0.944705271748769\n",
      "train loss:0.8150023006176496\n",
      "train loss:1.0235390370535007\n",
      "train loss:0.9724467773027641\n",
      "train loss:1.0584257007562143\n",
      "train loss:0.9669016816289493\n",
      "train loss:0.7783035265450898\n",
      "train loss:0.8559421169383953\n",
      "train loss:0.8328872370459846\n",
      "train loss:1.0557199051835955\n",
      "train loss:0.9706882853212635\n",
      "train loss:0.9109603004473819\n",
      "train loss:1.192138623883963\n",
      "train loss:0.9983483808591889\n",
      "train loss:0.8507409684105187\n",
      "train loss:1.1216423110324438\n",
      "train loss:0.8397008431736634\n",
      "train loss:1.0512907541650822\n",
      "train loss:1.1464977044258358\n",
      "train loss:1.0717392617614914\n",
      "train loss:0.9794247725659448\n",
      "train loss:1.0051654240030745\n",
      "train loss:0.9254734549519033\n",
      "train loss:1.0505721977213276\n",
      "train loss:1.0383409988799766\n",
      "train loss:1.013177133326523\n",
      "train loss:0.9520764105608002\n",
      "train loss:0.753318571771251\n",
      "train loss:0.8090131164112825\n",
      "train loss:1.064916368143391\n",
      "train loss:1.0892098048592755\n",
      "train loss:0.9673957486913557\n",
      "train loss:1.129721250068166\n",
      "train loss:0.981106634839248\n",
      "train loss:1.024279922975073\n",
      "train loss:1.0660033978784016\n",
      "train loss:0.8457719821034315\n",
      "train loss:1.0590454093008825\n",
      "train loss:0.9824011328397106\n",
      "train loss:0.9636397952151775\n",
      "train loss:1.1006612137763836\n",
      "train loss:1.2156963851165279\n",
      "train loss:1.1883732404525953\n",
      "train loss:1.0834620816211127\n",
      "train loss:1.1247909425661964\n",
      "train loss:1.0424131477875849\n",
      "train loss:1.0398692171082313\n",
      "train loss:1.0310665102489074\n",
      "train loss:0.9657321770988664\n",
      "train loss:0.9625534908596606\n",
      "train loss:0.8882285765455238\n",
      "train loss:0.9910651640392232\n",
      "train loss:1.0424968812233955\n",
      "train loss:1.1189806035759575\n",
      "train loss:0.6902016483408987\n",
      "train loss:1.2011038443152413\n",
      "train loss:1.036446228861635\n",
      "train loss:1.0006346987234602\n",
      "train loss:0.9641691763452686\n",
      "train loss:1.0763069070892124\n",
      "train loss:0.9648185873840283\n",
      "train loss:0.9907738099402273\n",
      "train loss:0.8755432136464343\n",
      "train loss:1.2374076669525238\n",
      "train loss:1.1540491907759312\n",
      "train loss:0.8115901157764255\n",
      "train loss:0.9188300822757075\n",
      "train loss:1.1359867423738255\n",
      "train loss:1.0163622246496273\n",
      "train loss:1.009938503331946\n",
      "train loss:0.9248143442245422\n",
      "train loss:1.1246549045801724\n",
      "train loss:0.9577835362052105\n",
      "train loss:0.9395010260562436\n",
      "train loss:1.0531677123361072\n",
      "train loss:0.9822093685597458\n",
      "train loss:0.929788011946897\n",
      "train loss:0.8624164952259349\n",
      "train loss:0.9706757015080351\n",
      "train loss:1.0891871799170425\n",
      "train loss:1.1235395617404411\n",
      "train loss:0.9966746998979683\n",
      "train loss:0.8381494935387594\n",
      "train loss:1.0810500430891252\n",
      "train loss:1.1337863354852873\n",
      "train loss:1.0916411155264472\n",
      "train loss:0.9204493700787336\n",
      "train loss:1.0038089330647444\n",
      "train loss:1.065876703120597\n",
      "train loss:0.996190271282707\n",
      "train loss:1.030053804583074\n",
      "train loss:0.9893474868291091\n",
      "train loss:1.049621594204315\n",
      "train loss:0.9583365929303475\n",
      "train loss:0.8792531363807063\n",
      "train loss:1.0640586427549408\n",
      "train loss:0.8921003100888738\n",
      "train loss:1.0410963668696478\n",
      "train loss:0.8661725786535984\n",
      "train loss:1.122588823521726\n",
      "train loss:0.9932085551410671\n",
      "train loss:0.8397406938926075\n",
      "train loss:0.9641840472198209\n",
      "train loss:0.918030420286606\n",
      "train loss:1.06242400581123\n",
      "train loss:1.0531197434142343\n",
      "train loss:1.0435627850922755\n",
      "train loss:1.12053029136215\n",
      "train loss:1.1020773258084824\n",
      "train loss:0.9783593971806657\n",
      "train loss:0.9080937728377215\n",
      "train loss:0.8899978594285537\n",
      "train loss:1.1613814547911605\n",
      "train loss:1.0593335204387702\n",
      "train loss:0.9597169647056603\n",
      "train loss:0.8083913892976092\n",
      "train loss:0.8820124642664744\n",
      "train loss:0.9546161188971685\n",
      "train loss:0.9739934343469101\n",
      "train loss:0.9969077203072898\n",
      "train loss:1.1910657287638033\n",
      "train loss:0.9741177854926417\n",
      "train loss:0.9667300451232425\n",
      "train loss:1.1007746001116725\n",
      "train loss:0.8848275899112511\n",
      "train loss:1.147454627211203\n",
      "train loss:0.8982453362449\n",
      "train loss:1.1091295343152898\n",
      "train loss:0.9555695417021411\n",
      "train loss:0.9963148274935075\n",
      "train loss:0.8280733552529631\n",
      "train loss:1.0620233173628477\n",
      "train loss:0.8737702039811355\n",
      "train loss:1.0273796427158755\n",
      "train loss:0.9505010396498277\n",
      "train loss:0.9163458071421955\n",
      "train loss:1.0022868908578793\n",
      "train loss:1.1347024489896358\n",
      "train loss:1.1261926700283782\n",
      "train loss:0.9345190375418352\n",
      "train loss:0.9465097168694072\n",
      "train loss:1.043200028662995\n",
      "train loss:1.124674049228268\n",
      "train loss:0.917550137644442\n",
      "train loss:1.2311900397365925\n",
      "train loss:0.8674021343204223\n",
      "train loss:0.7797058142010013\n",
      "train loss:1.2461907993125947\n",
      "train loss:1.1014272098457867\n",
      "train loss:0.9275879450430922\n",
      "train loss:0.9165348371862051\n",
      "train loss:1.1473258108443405\n",
      "train loss:1.000116146396306\n",
      "train loss:0.9904024092605239\n",
      "train loss:0.9393686808383356\n",
      "train loss:0.8982068111679985\n",
      "train loss:0.9627493469749885\n",
      "train loss:0.888738943790726\n",
      "train loss:0.9945225007298714\n",
      "train loss:0.9860707412692622\n",
      "train loss:0.933131874406007\n",
      "train loss:0.9794377235576753\n",
      "train loss:0.958344418733108\n",
      "train loss:0.8622903687261204\n",
      "train loss:1.1309198224352228\n",
      "train loss:1.0128736589348937\n",
      "train loss:0.9573143998776104\n",
      "train loss:0.9599701098491322\n",
      "train loss:0.9816756257849636\n",
      "train loss:1.1156804810667587\n",
      "train loss:1.0647880889264294\n",
      "train loss:0.8521595107928264\n",
      "train loss:1.0317937677669629\n",
      "train loss:0.9629954192543648\n",
      "train loss:1.1576453539514653\n",
      "train loss:1.0994948829297888\n",
      "train loss:1.1529829319249252\n",
      "train loss:1.0221331853597384\n",
      "train loss:1.1079872604185672\n",
      "train loss:0.9566057696187948\n",
      "train loss:0.9072903906338735\n",
      "train loss:0.9387940542946839\n",
      "train loss:1.146833694962484\n",
      "train loss:1.003475822607451\n",
      "train loss:0.9825678496381086\n",
      "train loss:0.949285895693862\n",
      "train loss:1.1071357276892542\n",
      "train loss:1.053788561439201\n",
      "train loss:1.1794001935877754\n",
      "train loss:0.9208726344364203\n",
      "train loss:0.9578292029531446\n",
      "train loss:1.0360635360142598\n",
      "train loss:1.0399610669379948\n",
      "train loss:0.7702002801611887\n",
      "train loss:0.8999753650467059\n",
      "train loss:0.9804553650997214\n",
      "train loss:1.0783858684869765\n",
      "train loss:0.8332835425580839\n",
      "train loss:1.002794102796916\n",
      "train loss:1.0625035607057405\n",
      "train loss:0.9186579737035617\n",
      "train loss:0.9248288341883064\n",
      "train loss:0.9766401058077154\n",
      "train loss:1.0022735891344208\n",
      "train loss:0.9627921052181169\n",
      "train loss:1.1617867639373485\n",
      "train loss:1.113328014217808\n",
      "train loss:1.1380344765559263\n",
      "train loss:0.9406936490657098\n",
      "train loss:0.9633020768306878\n",
      "train loss:0.9430290415486362\n",
      "train loss:0.8726009641618208\n",
      "train loss:0.9513228053966624\n",
      "train loss:1.0355505255335948\n",
      "train loss:1.0349029206795808\n",
      "train loss:1.1513041746160226\n",
      "train loss:1.1003967179734293\n",
      "train loss:0.9932008170266077\n",
      "train loss:0.9099216892085047\n",
      "train loss:0.9328464099288029\n",
      "train loss:1.1100742567423663\n",
      "train loss:0.9266551318095081\n",
      "train loss:0.9514818523960885\n",
      "train loss:0.8872227889284866\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 코드\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist\n",
    "\n",
    "from trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
